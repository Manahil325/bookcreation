"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[7442],{5299(n,a,e){e.r(a),e.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>s,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-2-digital-twins/chapter-3-sensor-validation/comparison-tools","title":"Comparison Tools","description":"Tools for comparing simulation vs real data in sensor validation","source":"@site/docs/module-2-digital-twins/chapter-3-sensor-validation/comparison-tools.md","sourceDirName":"module-2-digital-twins/chapter-3-sensor-validation","slug":"/module-2-digital-twins/chapter-3-sensor-validation/comparison-tools","permalink":"/docs/module-2-digital-twins/chapter-3-sensor-validation/comparison-tools","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-2-digital-twins/chapter-3-sensor-validation/comparison-tools.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Comparison Tools","sidebar_position":3,"description":"Tools for comparing simulation vs real data in sensor validation","keywords":["comparison tools","sensor validation","robotics","data analysis"],"learning_objectives":["Understand tools for comparing simulation and real data","Learn how to visualize and analyze sensor data","Know how to use validation frameworks","Implement automated comparison systems"],"estimated_time":"2 hours","difficulty":"Advanced","prerequisites":["Validation methods concepts","Basic data analysis skills"]},"sidebar":"tutorialSidebar","previous":{"title":"Validation Methods","permalink":"/docs/module-2-digital-twins/chapter-3-sensor-validation/validation-methods"},"next":{"title":"Chapter 3 Exercises","permalink":"/docs/module-2-digital-twins/chapter-3-sensor-validation/exercises"}}');var t=e(4848),o=e(8453);const s={title:"Comparison Tools",sidebar_position:3,description:"Tools for comparing simulation vs real data in sensor validation",keywords:["comparison tools","sensor validation","robotics","data analysis"],learning_objectives:["Understand tools for comparing simulation and real data","Learn how to visualize and analyze sensor data","Know how to use validation frameworks","Implement automated comparison systems"],estimated_time:"2 hours",difficulty:"Advanced",prerequisites:["Validation methods concepts","Basic data analysis skills"]},r="Comparison Tools",l={},d=[{value:"Overview of Comparison Tools",id:"overview-of-comparison-tools",level:2},{value:"Types of Comparison Tools",id:"types-of-comparison-tools",level:3},{value:"Comparison Requirements",id:"comparison-requirements",level:3},{value:"Visual Analysis Tools",id:"visual-analysis-tools",level:2},{value:"Side-by-Side Comparison",id:"side-by-side-comparison",level:3},{value:"Overlay Visualization",id:"overlay-visualization",level:3},{value:"Statistical Analysis Tools",id:"statistical-analysis-tools",level:2},{value:"Data Synchronization",id:"data-synchronization",level:3},{value:"Statistical Metrics Calculator",id:"statistical-metrics-calculator",level:3},{value:"Automated Comparison Framework",id:"automated-comparison-framework",level:2},{value:"Continuous Validation System",id:"continuous-validation-system",level:3},{value:"Data Visualization Tools",id:"data-visualization-tools",level:2},{value:"Real-time Comparison Dashboard",id:"real-time-comparison-dashboard",level:3},{value:"Specialized Comparison Tools",id:"specialized-comparison-tools",level:2},{value:"LIDAR Comparison Tool",id:"lidar-comparison-tool",level:3},{value:"Best Practices for Comparison",id:"best-practices-for-comparison",level:2},{value:"Exercise",id:"exercise",level:2}];function c(n){const a={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(a.header,{children:(0,t.jsx)(a.h1,{id:"comparison-tools",children:"Comparison Tools"})}),"\n",(0,t.jsx)(a.p,{children:"Comparing simulation data with real-world data is essential for validating sensor models in digital twins. This section covers various tools and techniques for performing these comparisons effectively."}),"\n",(0,t.jsx)(a.h2,{id:"overview-of-comparison-tools",children:"Overview of Comparison Tools"}),"\n",(0,t.jsx)(a.h3,{id:"types-of-comparison-tools",children:"Types of Comparison Tools"}),"\n",(0,t.jsxs)(a.ol,{children:["\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.strong,{children:"Visual Analysis Tools"}),": For manual inspection of data"]}),"\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.strong,{children:"Statistical Analysis Tools"}),": For quantitative validation"]}),"\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.strong,{children:"Automated Validation Tools"}),": For continuous validation"]}),"\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.strong,{children:"Visualization Tools"}),": For understanding data patterns"]}),"\n"]}),"\n",(0,t.jsx)(a.h3,{id:"comparison-requirements",children:"Comparison Requirements"}),"\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.strong,{children:"Synchronization"}),": Aligning data from different sources"]}),"\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.strong,{children:"Normalization"}),": Converting data to comparable formats"]}),"\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.strong,{children:"Filtering"}),": Removing noise and outliers"]}),"\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.strong,{children:"Metrics"}),": Quantifying differences between datasets"]}),"\n"]}),"\n",(0,t.jsx)(a.h2,{id:"visual-analysis-tools",children:"Visual Analysis Tools"}),"\n",(0,t.jsx)(a.h3,{id:"side-by-side-comparison",children:"Side-by-Side Comparison"}),"\n",(0,t.jsx)(a.p,{children:"Visual tools allow direct comparison of simulated vs real data:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-csharp",children:'using UnityEngine;\nusing UnityEngine.UI;\nusing System.Collections.Generic;\n\npublic class SideBySideComparison : MonoBehaviour\n{\n    [Header("UI References")]\n    public RawImage realImageDisplay;\n    public RawImage simulatedImageDisplay;\n    public Text comparisonInfo;\n    public Slider playbackSlider;\n\n    [Header("Data Sources")]\n    public List<Texture2D> realImages;\n    public List<Texture2D> simulatedImages;\n    public List<float> timestamps;\n\n    private int currentIndex = 0;\n\n    void Start()\n    {\n        UpdateDisplay();\n    }\n\n    void UpdateDisplay()\n    {\n        if (realImages.Count > currentIndex && simulatedImages.Count > currentIndex)\n        {\n            realImageDisplay.texture = realImages[currentIndex];\n            simulatedImageDisplay.texture = simulatedImages[currentIndex];\n\n            if (timestamps.Count > currentIndex)\n            {\n                comparisonInfo.text = $"Time: {timestamps[currentIndex]:F2}s | Index: {currentIndex + 1}/{realImages.Count}";\n            }\n        }\n    }\n\n    public void OnSliderChanged(float value)\n    {\n        currentIndex = Mathf.RoundToInt(value * (realImages.Count - 1));\n        UpdateDisplay();\n    }\n\n    public void NextFrame()\n    {\n        if (currentIndex < realImages.Count - 1)\n        {\n            currentIndex++;\n            float sliderValue = (float)currentIndex / (realImages.Count - 1);\n            playbackSlider.value = sliderValue;\n            UpdateDisplay();\n        }\n    }\n\n    public void PreviousFrame()\n    {\n        if (currentIndex > 0)\n        {\n            currentIndex--;\n            float sliderValue = (float)currentIndex / (realImages.Count - 1);\n            playbackSlider.value = sliderValue;\n            UpdateDisplay();\n        }\n    }\n}\n'})}),"\n",(0,t.jsx)(a.h3,{id:"overlay-visualization",children:"Overlay Visualization"}),"\n",(0,t.jsx)(a.p,{children:"Overlaying real and simulated data for direct comparison:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-csharp",children:'using UnityEngine;\nusing UnityEngine.UI;\nusing System.Collections.Generic;\n\npublic class OverlayVisualization : MonoBehaviour\n{\n    [Header("Canvas References")]\n    public RectTransform canvas;\n    public Color realColor = Color.red;\n    public Color simulatedColor = Color.blue;\n    public float lineThickness = 2.0f;\n\n    [Header("Data Sources")]\n    public List<Vector2> realData;  // Processed real sensor data\n    public List<Vector2> simulatedData;  // Processed simulated sensor data\n\n    private LineRenderer realLine;\n    private LineRenderer simulatedLine;\n\n    void Start()\n    {\n        CreateLineRenderers();\n    }\n\n    void CreateLineRenderers()\n    {\n        // Create real data line\n        GameObject realObj = new GameObject("RealDataLine");\n        realObj.transform.SetParent(canvas);\n        realLine = realObj.AddComponent<LineRenderer>();\n        ConfigureLineRenderer(realLine, realColor);\n\n        // Create simulated data line\n        GameObject simulatedObj = new GameObject("SimulatedDataLine");\n        simulatedObj.transform.SetParent(canvas);\n        simulatedLine = simulatedObj.AddComponent<LineRenderer>();\n        ConfigureLineRenderer(simulatedLine, simulatedColor);\n    }\n\n    void ConfigureLineRenderer(LineRenderer line, Color color)\n    {\n        line.material = new Material(Shader.Find("Sprites/Default"));\n        line.color = color;\n        line.startWidth = lineThickness;\n        line.endWidth = lineThickness;\n        line.useWorldSpace = false;\n    }\n\n    public void UpdateVisualization()\n    {\n        if (realData.Count > 0 && simulatedData.Count > 0)\n        {\n            SetLinePositions(realLine, realData);\n            SetLinePositions(simulatedLine, simulatedData);\n        }\n    }\n\n    void SetLinePositions(LineRenderer line, List<Vector2> data)\n    {\n        line.positionCount = data.Count;\n        Vector3[] positions = new Vector3[data.Count];\n\n        for (int i = 0; i < data.Count; i++)\n        {\n            // Convert data points to UI space\n            positions[i] = new Vector3(data[i].x, data[i].y, 0);\n        }\n\n        line.SetPositions(positions);\n    }\n\n    public void SetData(List<Vector2> real, List<Vector2> simulated)\n    {\n        realData = real;\n        simulatedData = simulated;\n        UpdateVisualization();\n    }\n}\n'})}),"\n",(0,t.jsx)(a.h2,{id:"statistical-analysis-tools",children:"Statistical Analysis Tools"}),"\n",(0,t.jsx)(a.h3,{id:"data-synchronization",children:"Data Synchronization"}),"\n",(0,t.jsx)(a.p,{children:"Aligning datasets for comparison:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-csharp",children:"using System.Collections.Generic;\nusing System.Linq;\n\npublic class DataSynchronizer : MonoBehaviour\n{\n    public class TimedData<T>\n    {\n        public float timestamp;\n        public T data;\n\n        public TimedData(float t, T d)\n        {\n            timestamp = t;\n            data = d;\n        }\n    }\n\n    public List<TimedData<T>> SynchronizeData<T>(\n        List<TimedData<T>> dataset1,\n        List<TimedData<T>> dataset2,\n        float maxTimeDifference = 0.01f)\n    {\n        List<TimedData<T>> synchronized1 = new List<TimedData<T>>();\n        List<TimedData<T>> synchronized2 = new List<TimedData<T>>();\n\n        foreach (var data1 in dataset1)\n        {\n            // Find closest timestamp in dataset2\n            TimedData<T> closest = FindClosestTimestamp(data1.timestamp, dataset2, maxTimeDifference);\n            if (closest != null)\n            {\n                synchronized1.Add(data1);\n                synchronized2.Add(closest);\n            }\n        }\n\n        return synchronized1; // Return synchronized pairs\n    }\n\n    TimedData<T> FindClosestTimestamp<T>(float targetTime, List<TimedData<T>> dataset, float maxDiff)\n    {\n        TimedData<T> closest = null;\n        float minDiff = float.MaxValue;\n\n        foreach (var data in dataset)\n        {\n            float diff = Mathf.Abs(data.timestamp - targetTime);\n            if (diff < minDiff && diff <= maxDiff)\n            {\n                minDiff = diff;\n                closest = data;\n            }\n        }\n\n        return closest;\n    }\n\n    public List<float> CalculateResiduals<T>(\n        List<TimedData<T>> dataset1,\n        List<TimedData<T>> dataset2,\n        System.Func<T, T, float> comparisonFunc)\n    {\n        List<float> residuals = new List<float>();\n\n        for (int i = 0; i < Mathf.Min(dataset1.Count, dataset2.Count); i++)\n        {\n            float residual = comparisonFunc(dataset1[i].data, dataset2[i].data);\n            residuals.Add(residual);\n        }\n\n        return residuals;\n    }\n}\n"})}),"\n",(0,t.jsx)(a.h3,{id:"statistical-metrics-calculator",children:"Statistical Metrics Calculator"}),"\n",(0,t.jsx)(a.p,{children:"Compute various statistical metrics for comparison:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-csharp",children:'using System.Collections.Generic;\nusing System.Linq;\n\npublic class StatisticalMetrics : MonoBehaviour\n{\n    public class ComparisonResult\n    {\n        public float meanError;\n        public float stdDeviation;\n        public float maxError;\n        public float minError;\n        public float medianError;\n        public float correlation;\n        public float rmse;\n        public float mae;\n        public float mape; // Mean Absolute Percentage Error\n    }\n\n    public ComparisonResult CalculateMetrics(List<float> real, List<float> simulated)\n    {\n        if (real.Count != simulated.Count || real.Count == 0)\n        {\n            Debug.LogError("Datasets must have the same size and not be empty");\n            return null;\n        }\n\n        List<float> errors = new List<float>();\n        for (int i = 0; i < real.Count; i++)\n        {\n            errors.Add(Mathf.Abs(real[i] - simulated[i]));\n        }\n\n        ComparisonResult result = new ComparisonResult\n        {\n            meanError = errors.Average(),\n            stdDeviation = CalculateStandardDeviation(errors),\n            maxError = errors.Max(),\n            minError = errors.Min(),\n            medianError = CalculateMedian(errors),\n            rmse = Mathf.Sqrt(errors.Select(e => e * e).Average()),\n            mae = errors.Average(),\n            correlation = CalculateCorrelation(real, simulated)\n        };\n\n        // Calculate MAPE (handle division by zero)\n        List<float> mapeValues = new List<float>();\n        for (int i = 0; i < real.Count; i++)\n        {\n            if (Mathf.Abs(real[i]) > float.Epsilon)\n            {\n                mapeValues.Add(Mathf.Abs((real[i] - simulated[i]) / real[i]) * 100);\n            }\n        }\n        result.mape = mapeValues.Count > 0 ? mapeValues.Average() : 0;\n\n        return result;\n    }\n\n    float CalculateStandardDeviation(List<float> values)\n    {\n        if (values.Count == 0) return 0;\n\n        float mean = values.Average();\n        float sum = values.Sum(v => Mathf.Pow(v - mean, 2));\n        return Mathf.Sqrt(sum / values.Count);\n    }\n\n    float CalculateMedian(List<float> values)\n    {\n        List<float> sorted = new List<float>(values);\n        sorted.Sort();\n        int count = sorted.Count;\n\n        if (count % 2 == 0)\n        {\n            return (sorted[count / 2 - 1] + sorted[count / 2]) / 2;\n        }\n        else\n        {\n            return sorted[count / 2];\n        }\n    }\n\n    float CalculateCorrelation(List<float> x, List<float> y)\n    {\n        if (x.Count != y.Count || x.Count == 0) return 0;\n\n        float meanX = x.Average();\n        float meanY = y.Average();\n\n        float numerator = 0;\n        float sumX2 = 0;\n        float sumY2 = 0;\n\n        for (int i = 0; i < x.Count; i++)\n        {\n            float diffX = x[i] - meanX;\n            float diffY = y[i] - meanY;\n\n            numerator += diffX * diffY;\n            sumX2 += diffX * diffX;\n            sumY2 += diffY * diffY;\n        }\n\n        float denominator = Mathf.Sqrt(sumX2 * sumY2);\n        return denominator != 0 ? numerator / denominator : 0;\n    }\n\n    public string FormatResults(ComparisonResult result)\n    {\n        return $"Mean Error: {result.meanError:F4}\\n" +\n               $"Std Dev: {result.stdDeviation:F4}\\n" +\n               $"Max Error: {result.maxError:F4}\\n" +\n               $"Min Error: {result.minError:F4}\\n" +\n               $"RMSE: {result.rmse:F4}\\n" +\n               $"MAE: {result.mae:F4}\\n" +\n               $"Correlation: {result.correlation:F4}\\n" +\n               $"MAPE: {result.mape:F2}%";\n    }\n}\n'})}),"\n",(0,t.jsx)(a.h2,{id:"automated-comparison-framework",children:"Automated Comparison Framework"}),"\n",(0,t.jsx)(a.h3,{id:"continuous-validation-system",children:"Continuous Validation System"}),"\n",(0,t.jsx)(a.p,{children:"An automated system for ongoing validation:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-csharp",children:'using System.Collections.Generic;\nusing System.IO;\nusing UnityEngine;\n\npublic class AutomatedValidationSystem : MonoBehaviour\n{\n    [Header("Validation Configuration")]\n    public float validationInterval = 5.0f; // Run validation every 5 seconds\n    public float accuracyThreshold = 0.05f;\n    public string resultsDirectory = "ValidationResults/";\n\n    [Header("Data Sources")]\n    public List<string> realDataTopics = new List<string>();\n    public List<string> simulatedDataTopics = new List<string>();\n\n    private float nextValidationTime;\n    private StatisticalMetrics metricsCalculator;\n    private List<ComparisonResult> historicalResults = new List<ComparisonResult>();\n\n    void Start()\n    {\n        metricsCalculator = GetComponent<StatisticalMetrics>();\n        nextValidationTime = Time.time + validationInterval;\n        EnsureResultsDirectory();\n    }\n\n    void Update()\n    {\n        if (Time.time >= nextValidationTime)\n        {\n            RunValidationCycle();\n            nextValidationTime = Time.time + validationInterval;\n        }\n    }\n\n    void RunValidationCycle()\n    {\n        Debug.Log("Running validation cycle...");\n\n        // Collect current data from real and simulated sources\n        List<float> realData = CollectRealData();\n        List<float> simulatedData = CollectSimulatedData();\n\n        if (realData.Count > 0 && simulatedData.Count > 0)\n        {\n            // Calculate metrics\n            ComparisonResult result = metricsCalculator.CalculateMetrics(realData, simulatedData);\n            if (result != null)\n            {\n                historicalResults.Add(result);\n\n                // Check if validation passed\n                bool passed = result.meanError <= accuracyThreshold;\n\n                // Log results\n                string resultText = metricsCalculator.FormatResults(result);\n                string status = passed ? "PASSED" : "FAILED";\n                Debug.Log($"Validation {status}: {resultText}");\n\n                // Save results\n                SaveValidationResult(result, passed);\n\n                // Trigger alerts if validation failed\n                if (!passed)\n                {\n                    OnValidationFailed(result);\n                }\n            }\n        }\n    }\n\n    List<float> CollectRealData()\n    {\n        // In a real implementation, this would collect data from real sensors\n        // via ROS bridge or other communication interface\n        List<float> data = new List<float>();\n\n        // Placeholder: generate some sample data\n        for (int i = 0; i < 100; i++)\n        {\n            data.Add(Random.Range(0.0f, 10.0f));\n        }\n\n        return data;\n    }\n\n    List<float> CollectSimulatedData()\n    {\n        // In a real implementation, this would collect data from simulated sensors\n        List<float> data = new List<float>();\n\n        // Placeholder: generate some sample data\n        for (int i = 0; i < 100; i++)\n        {\n            data.Add(Random.Range(0.0f, 10.0f) + 0.1f); // Slightly different from real data\n        }\n\n        return data;\n    }\n\n    void EnsureResultsDirectory()\n    {\n        if (!Directory.Exists(resultsDirectory))\n        {\n            Directory.CreateDirectory(resultsDirectory);\n        }\n    }\n\n    void SaveValidationResult(ComparisonResult result, bool passed)\n    {\n        string filename = Path.Combine(resultsDirectory, $"validation_{System.DateTime.Now:yyyyMMdd_HHmmss}.txt");\n\n        string content = $"Validation Result - {System.DateTime.Now}\\n" +\n                        $"Status: {(passed ? "PASSED" : "FAILED")}\\n" +\n                        $"Threshold: {accuracyThreshold}\\n" +\n                        metricsCalculator.FormatResults(result);\n\n        File.WriteAllText(filename, content);\n        Debug.Log($"Validation result saved to: {filename}");\n    }\n\n    void OnValidationFailed(ComparisonResult result)\n    {\n        Debug.LogWarning($"Validation failed! Mean Error: {result.meanError}, Threshold: {accuracyThreshold}");\n\n        // In a real system, you might:\n        // - Send an alert to operators\n        // - Adjust simulation parameters\n        // - Trigger recalibration procedures\n        // - Log detailed diagnostic information\n    }\n\n    public float GetHistoricalAccuracy()\n    {\n        if (historicalResults.Count == 0) return 0;\n\n        int passedCount = 0;\n        foreach (var result in historicalResults)\n        {\n            if (result.meanError <= accuracyThreshold) passedCount++;\n        }\n\n        return (float)passedCount / historicalResults.Count;\n    }\n}\n'})}),"\n",(0,t.jsx)(a.h2,{id:"data-visualization-tools",children:"Data Visualization Tools"}),"\n",(0,t.jsx)(a.h3,{id:"real-time-comparison-dashboard",children:"Real-time Comparison Dashboard"}),"\n",(0,t.jsx)(a.p,{children:"A dashboard for monitoring validation in real-time:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-csharp",children:'using UnityEngine;\nusing UnityEngine.UI;\nusing System.Collections.Generic;\n\npublic class ValidationDashboard : MonoBehaviour\n{\n    [Header("UI References")]\n    public Text statusText;\n    public Text metricsText;\n    public Slider accuracySlider;\n    public Image statusIndicator;\n    public GameObject historyGraph;\n\n    [Header("Validation System")]\n    public AutomatedValidationSystem validationSystem;\n    public StatisticalMetrics metricsCalculator;\n\n    [Header("Color Settings")]\n    public Color goodColor = Color.green;\n    public Color warningColor = Color.yellow;\n    public Color errorColor = Color.red;\n\n    private List<float> accuracyHistory = new List<float>();\n    private const int maxHistoryPoints = 50;\n\n    void Start()\n    {\n        UpdateDashboard();\n    }\n\n    void Update()\n    {\n        UpdateDashboard();\n    }\n\n    void UpdateDashboard()\n    {\n        if (validationSystem != null)\n        {\n            float historicalAccuracy = validationSystem.GetHistoricalAccuracy();\n            UpdateAccuracyDisplay(historicalAccuracy);\n        }\n    }\n\n    void UpdateAccuracyDisplay(float accuracy)\n    {\n        // Update status text\n        statusText.text = $"Validation Accuracy: {(accuracy * 100):F1}%";\n\n        // Update slider\n        accuracySlider.value = accuracy;\n\n        // Update status indicator color\n        if (accuracy >= 0.95f)\n        {\n            statusIndicator.color = goodColor;\n        }\n        else if (accuracy >= 0.80f)\n        {\n            statusIndicator.color = warningColor;\n        }\n        else\n        {\n            statusIndicator.color = errorColor;\n        }\n\n        // Add to history for graphing\n        accuracyHistory.Add(accuracy);\n        if (accuracyHistory.Count > maxHistoryPoints)\n        {\n            accuracyHistory.RemoveAt(0);\n        }\n\n        UpdateHistoryGraph();\n    }\n\n    void UpdateHistoryGraph()\n    {\n        // In a real implementation, this would update a graph visualization\n        // showing the history of validation accuracy over time\n        Debug.Log($"Updated accuracy history with {accuracyHistory.Count} points");\n    }\n\n    public void ExportResults()\n    {\n        // Export validation results to CSV or other format\n        string csvContent = "Timestamp,Accuracy,MeanError,RMSE,Correlation\\n";\n\n        // This would be populated with actual validation results\n        // over time for external analysis\n\n        Debug.Log("Exported validation results");\n    }\n\n    public void ResetHistory()\n    {\n        accuracyHistory.Clear();\n        if (validationSystem != null)\n        {\n            validationSystem.historicalResults.Clear();\n        }\n        UpdateDashboard();\n    }\n}\n'})}),"\n",(0,t.jsx)(a.h2,{id:"specialized-comparison-tools",children:"Specialized Comparison Tools"}),"\n",(0,t.jsx)(a.h3,{id:"lidar-comparison-tool",children:"LIDAR Comparison Tool"}),"\n",(0,t.jsx)(a.p,{children:"Specifically for comparing LIDAR data:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-csharp",children:"using UnityEngine;\nusing System.Collections.Generic;\n\npublic class LIDARComparisonTool : MonoBehaviour\n{\n    public class LIDARPoint\n    {\n        public float angle;\n        public float range;\n        public float intensity;\n    }\n\n    public float CompareLIDARScans(List<LIDARPoint> realScan, List<LIDARPoint> simulatedScan)\n    {\n        // Align scans by angle\n        Dictionary<float, LIDARPoint> realDict = new Dictionary<float, LIDARPoint>();\n        Dictionary<float, LIDARPoint> simulatedDict = new Dictionary<float, LIDARPoint>();\n\n        foreach (var point in realScan)\n        {\n            realDict[Mathf.Round(point.angle * 1000) / 1000] = point; // Round to avoid floating point issues\n        }\n\n        foreach (var point in simulatedScan)\n        {\n            simulatedDict[Mathf.Round(point.angle * 1000) / 1000] = point;\n        }\n\n        // Calculate comparison metrics\n        List<float> rangeErrors = new List<float>();\n        List<float> intensityErrors = new List<float>();\n\n        foreach (var kvp in realDict)\n        {\n            float angle = kvp.Key;\n            LIDARPoint realPoint = kvp.Value;\n\n            if (simulatedDict.ContainsKey(angle))\n            {\n                LIDARPoint simPoint = simulatedDict[angle];\n                rangeErrors.Add(Mathf.Abs(realPoint.range - simPoint.range));\n                intensityErrors.Add(Mathf.Abs(realPoint.intensity - simPoint.intensity));\n            }\n        }\n\n        // Calculate average error\n        float avgRangeError = rangeErrors.Count > 0 ? rangeErrors.Average() : float.MaxValue;\n        float avgIntensityError = intensityErrors.Count > 0 ? intensityErrors.Average() : float.MaxValue;\n\n        // Return a combined error metric\n        return (avgRangeError + avgIntensityError) / 2.0f;\n    }\n\n    public float CalculateLIDARSimilarity(List<LIDARPoint> scan1, List<LIDARPoint> scan2)\n    {\n        // Calculate similarity using histogram comparison\n        var hist1 = CreateLIDARHistogram(scan1);\n        var hist2 = CreateLIDARHistogram(scan2);\n\n        // Calculate histogram intersection\n        float intersection = 0;\n        for (int i = 0; i < hist1.Count && i < hist2.Count; i++)\n        {\n            intersection += Mathf.Min(hist1[i], hist2[i]);\n        }\n\n        return intersection;\n    }\n\n    List<float> CreateLIDARHistogram(List<LIDARPoint> scan, int bins = 360)\n    {\n        List<float> histogram = new List<float>(new float[bins]);\n\n        foreach (var point in scan)\n        {\n            int binIndex = Mathf.Clamp(Mathf.RoundToInt(point.angle * bins / (2 * Mathf.PI)), 0, bins - 1);\n            histogram[binIndex] += point.range; // Weight by range\n        }\n\n        // Normalize\n        float sum = histogram.Sum();\n        if (sum > 0)\n        {\n            for (int i = 0; i < histogram.Count; i++)\n            {\n                histogram[i] /= sum;\n            }\n        }\n\n        return histogram;\n    }\n}\n"})}),"\n",(0,t.jsx)(a.h2,{id:"best-practices-for-comparison",children:"Best Practices for Comparison"}),"\n",(0,t.jsxs)(a.ol,{children:["\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.strong,{children:"Data Synchronization"}),": Ensure timestamps are properly aligned"]}),"\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.strong,{children:"Normalization"}),": Convert data to comparable units and scales"]}),"\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.strong,{children:"Statistical Rigor"}),": Use appropriate statistical tests"]}),"\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.strong,{children:"Visualization"}),": Always visualize data before relying on statistics"]}),"\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.strong,{children:"Documentation"}),": Keep detailed records of comparison methods and results"]}),"\n"]}),"\n",(0,t.jsx)(a.h2,{id:"exercise",children:"Exercise"}),"\n",(0,t.jsx)(a.p,{children:"Create a comparison tool for a specific sensor type (e.g., camera, LIDAR, or IMU). Your tool should include:"}),"\n",(0,t.jsxs)(a.ol,{children:["\n",(0,t.jsx)(a.li,{children:"Visual comparison interface showing real vs simulated data"}),"\n",(0,t.jsx)(a.li,{children:"Statistical analysis with multiple metrics"}),"\n",(0,t.jsx)(a.li,{children:"Automated validation with threshold checking"}),"\n",(0,t.jsx)(a.li,{children:"Results reporting and visualization"}),"\n"]}),"\n",(0,t.jsx)(a.p,{children:"Test your tool with sample datasets and document the comparison results."})]})}function u(n={}){const{wrapper:a}={...(0,o.R)(),...n.components};return a?(0,t.jsx)(a,{...n,children:(0,t.jsx)(c,{...n})}):c(n)}},8453(n,a,e){e.d(a,{R:()=>s,x:()=>r});var i=e(6540);const t={},o=i.createContext(t);function s(n){const a=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(a):{...a,...n}},[a,n])}function r(n){let a;return a=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:s(n.components),i.createElement(o.Provider,{value:a},n.children)}}}]);